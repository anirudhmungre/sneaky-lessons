{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## Install Java, Spark, and Findspark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.3.2-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Spark and start session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"evr_nlp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   class|                text|\n",
      "+--------+--------------------+\n",
      "|positive|Wow... Loved this...|\n",
      "|negative|  Crust is not good.|\n",
      "|negative|Not tasty and the...|\n",
      "|positive|Stopped by during...|\n",
      "|positive|The selection on ...|\n",
      "|negative|Now I am getting ...|\n",
      "|negative|Honeslty it didn'...|\n",
      "|negative|The potatoes were...|\n",
      "|positive|The fries were gr...|\n",
      "|positive|      A great touch.|\n",
      "|positive|Service was very ...|\n",
      "|negative|  Would not go back.|\n",
      "|negative|The cashier had n...|\n",
      "|positive|I tried the Cape ...|\n",
      "|negative|I was disgusted b...|\n",
      "|negative|I was shocked bec...|\n",
      "|positive| Highly recommended.|\n",
      "|negative|Waitress was a li...|\n",
      "|negative|This place is not...|\n",
      "|negative|did not like at all.|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------+\n",
      "|   class|                text|length|\n",
      "+--------+--------------------+------+\n",
      "|positive|Wow... Loved this...|    24|\n",
      "|negative|  Crust is not good.|    18|\n",
      "|negative|Not tasty and the...|    41|\n",
      "|positive|Stopped by during...|    87|\n",
      "|positive|The selection on ...|    59|\n",
      "|negative|Now I am getting ...|    46|\n",
      "|negative|Honeslty it didn'...|    37|\n",
      "|negative|The potatoes were...|   111|\n",
      "|positive|The fries were gr...|    25|\n",
      "|positive|      A great touch.|    14|\n",
      "|positive|Service was very ...|    24|\n",
      "|negative|  Would not go back.|    18|\n",
      "|negative|The cashier had n...|    99|\n",
      "|positive|I tried the Cape ...|    59|\n",
      "|negative|I was disgusted b...|    62|\n",
      "|negative|I was shocked bec...|    50|\n",
      "|positive| Highly recommended.|    19|\n",
      "|negative|Waitress was a li...|    38|\n",
      "|negative|This place is not...|    51|\n",
      "|negative|did not like at all.|    20|\n",
      "+--------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a length column to be used as a future feature \n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "\n",
    "# Create the data processing pipeline functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(262145,[33933,69...|\n",
      "|  1.0|(262145,[15889,13...|\n",
      "|  1.0|(262145,[25570,63...|\n",
      "|  0.0|(262145,[6286,272...|\n",
      "|  0.0|(262145,[6979,255...|\n",
      "|  1.0|(262145,[24417,24...|\n",
      "|  1.0|(262145,[12084,48...|\n",
      "|  1.0|(262145,[3645,963...|\n",
      "|  0.0|(262145,[53777,10...|\n",
      "|  0.0|(262145,[138356,2...|\n",
      "|  0.0|(262145,[24113,25...|\n",
      "|  1.0|(262145,[68867,13...|\n",
      "|  1.0|(262145,[24417,36...|\n",
      "|  0.0|(262145,[18098,24...|\n",
      "|  1.0|(262145,[24417,25...|\n",
      "|  1.0|(262145,[24417,25...|\n",
      "|  0.0|(262145,[31704,21...|\n",
      "|  1.0|(262145,[25570,27...|\n",
      "|  1.0|(262145,[12329,15...|\n",
      "|  1.0|(262145,[8287,139...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show label of and resulting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break data down into a training set and a testing set\n",
    "(training, testing) = cleaned.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Create a Naive Bayes model and fit training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|   class|                text|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|negative|              #NAME?|     6|  1.0|            [#name?]|            [#name?]|(262144,[75443],[...|(262144,[75443],[...|(262145,[75443,26...|[-70.909870435424...|[0.94433354425384...|       0.0|\n",
      "|negative|2 times - Very Ba...|    37|  1.0|[2, times, -, ver...|[2, times, -, bad...|(262144,[24113,28...|(262144,[24113,28...|(262145,[24113,28...|[-469.63179477916...|[7.85369069301455...|       1.0|\n",
      "|negative|A lady at the tab...|    75|  1.0|[a, lady, at, the...|[lady, table, nex...|(262144,[21872,33...|(262144,[21872,33...|(262145,[21872,33...|[-895.44955237719...|[1.79726596336076...|       1.0|\n",
      "|negative|After I pulled up...|    83|  1.0|[after, i, pulled...|[pulled, car, wai...|(262144,[16332,24...|(262144,[16332,24...|(262145,[16332,24...|[-926.97060375006...|[1.67506770105780...|       1.0|\n",
      "|negative|After all the rav...|    82|  1.0|[after, all, the,...|[rave, reviews, w...|(262144,[22808,24...|(262144,[22808,24...|(262145,[22808,24...|[-805.82685070437...|[3.20969770368397...|       1.0|\n",
      "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tranform the model with the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting reviews was: 0.7322874998838077\n"
     ]
    }
   ],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(f\"Accuracy of model at predicting reviews was: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nteract": {
   "version": "0.11.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
